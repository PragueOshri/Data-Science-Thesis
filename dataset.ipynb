{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e059740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from numpy.random import seed\n",
    "import os\n",
    "from scipy import stats\n",
    "from numpy.random import randint\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3ab98b",
   "metadata": {},
   "source": [
    "# Read Questionnaires data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53b5782f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DistributionChannel</th>\n",
       "      <th>Duration (in seconds)</th>\n",
       "      <th>EndDate</th>\n",
       "      <th>Finished</th>\n",
       "      <th>Progress</th>\n",
       "      <th>RecordedDate</th>\n",
       "      <th>ResponseId</th>\n",
       "      <th>StartDate</th>\n",
       "      <th>Status</th>\n",
       "      <th>UserLanguage</th>\n",
       "      <th>...</th>\n",
       "      <th>war_jobStatusBefore</th>\n",
       "      <th>war_newsConsumption</th>\n",
       "      <th>war_newsResources</th>\n",
       "      <th>war_servForce</th>\n",
       "      <th>war_servForce2</th>\n",
       "      <th>war_servForce3</th>\n",
       "      <th>war_servForce3other</th>\n",
       "      <th>war_worry2someone</th>\n",
       "      <th>war_worry2someone2</th>\n",
       "      <th>war_worry2someone3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Distribution Channel</td>\n",
       "      <td>Duration (in seconds)</td>\n",
       "      <td>End Date</td>\n",
       "      <td>Finished</td>\n",
       "      <td>Progress</td>\n",
       "      <td>Recorded Date</td>\n",
       "      <td>Response ID</td>\n",
       "      <td>Start Date</td>\n",
       "      <td>Response Type</td>\n",
       "      <td>User Language</td>\n",
       "      <td>...</td>\n",
       "      <td>מצב תעסוקתי (בשגרה, לפני ה- 7/10/23)</td>\n",
       "      <td>כמה זמן את/ה מבלה מול החדשות ביום</td>\n",
       "      <td>מה מקור/ות החדשות העיקרי/ם שלך (ניתן לסמן כמה ...</td>\n",
       "      <td>האם שרת/ת בשירות צבאי/מילואים/כוחות הביטחון מא...</td>\n",
       "      <td>אורך השירות במלחמה (יש לציין מספר שבועות או מס...</td>\n",
       "      <td>תפקידי בשירות הינו:</td>\n",
       "      <td>במידה וסימנת אחר, נשמח שתפרט/י במידת האפשר</td>\n",
       "      <td>האם יש א/נשים ספציפיים במעגל קרוב שאת/ה דואג/ת...</td>\n",
       "      <td>עד כמה הדאגה לא/נשים ספציפיים מעסיקה אותה במהל...</td>\n",
       "      <td>עד כמה הדאגה לא/נשים ספציפיים מפריעה לך לישון ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{\"ImportId\":\"distributionChannel\"}</td>\n",
       "      <td>{\"ImportId\":\"duration\"}</td>\n",
       "      <td>{\"ImportId\":\"endDate\",\"timeZone\":\"Europe/Athens\"}</td>\n",
       "      <td>{\"ImportId\":\"finished\"}</td>\n",
       "      <td>{\"ImportId\":\"progress\"}</td>\n",
       "      <td>{\"ImportId\":\"recordedDate\",\"timeZone\":\"Europe/...</td>\n",
       "      <td>{\"ImportId\":\"_recordId\"}</td>\n",
       "      <td>{\"ImportId\":\"startDate\",\"timeZone\":\"Europe/Ath...</td>\n",
       "      <td>{\"ImportId\":\"status\"}</td>\n",
       "      <td>{\"ImportId\":\"userLanguage\"}</td>\n",
       "      <td>...</td>\n",
       "      <td>{\"ImportId\":\"QID1215542841\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215542845\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215542846\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215660416\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215660417_TEXT\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215660418\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215660419_TEXT\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215542851\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215542852\"}</td>\n",
       "      <td>{\"ImportId\":\"QID1215542853\"}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>anonymous</td>\n",
       "      <td>619</td>\n",
       "      <td>2023-12-17 17:17:18</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>2023-12-17 17:17:19</td>\n",
       "      <td>R_1R6hR2rt6cWkp6F</td>\n",
       "      <td>2023-12-17 17:06:58</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>HE</td>\n",
       "      <td>...</td>\n",
       "      <td>משרה חלקית</td>\n",
       "      <td>עד 2 שעות</td>\n",
       "      <td>רדיו,אתרי חדשות,מדיה חברתית</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>כן</td>\n",
       "      <td>במידה רבה</td>\n",
       "      <td>במידה בינונית</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>anonymous</td>\n",
       "      <td>947</td>\n",
       "      <td>2024-01-03 16:20:02</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-01-03 16:20:03</td>\n",
       "      <td>R_2RdYEoBoKYX29sa</td>\n",
       "      <td>2024-01-03 16:04:15</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>HE</td>\n",
       "      <td>...</td>\n",
       "      <td>משרה חלקית</td>\n",
       "      <td>עד 2 שעות</td>\n",
       "      <td>רדיו,טלוויזיה</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>כן</td>\n",
       "      <td>במידה רבה</td>\n",
       "      <td>במידה בינונית</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>anonymous</td>\n",
       "      <td>610</td>\n",
       "      <td>2024-01-07 12:00:50</td>\n",
       "      <td>True</td>\n",
       "      <td>100</td>\n",
       "      <td>2024-01-07 12:00:51</td>\n",
       "      <td>R_2r8MhgLCoEhClwL</td>\n",
       "      <td>2024-01-07 11:50:39</td>\n",
       "      <td>IP Address</td>\n",
       "      <td>HE</td>\n",
       "      <td>...</td>\n",
       "      <td>משרה חלקית</td>\n",
       "      <td>עד 2 שעות</td>\n",
       "      <td>אתרי חדשות,מדיה חברתית</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>כן</td>\n",
       "      <td>במידה בינונית</td>\n",
       "      <td>כלל לא</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   DistributionChannel    Duration (in seconds)  \\\n",
       "0                 Distribution Channel    Duration (in seconds)   \n",
       "1   {\"ImportId\":\"distributionChannel\"}  {\"ImportId\":\"duration\"}   \n",
       "19                           anonymous                      619   \n",
       "20                           anonymous                      947   \n",
       "21                           anonymous                      610   \n",
       "\n",
       "                                              EndDate  \\\n",
       "0                                            End Date   \n",
       "1   {\"ImportId\":\"endDate\",\"timeZone\":\"Europe/Athens\"}   \n",
       "19                                2023-12-17 17:17:18   \n",
       "20                                2024-01-03 16:20:02   \n",
       "21                                2024-01-07 12:00:50   \n",
       "\n",
       "                   Finished                 Progress  \\\n",
       "0                  Finished                 Progress   \n",
       "1   {\"ImportId\":\"finished\"}  {\"ImportId\":\"progress\"}   \n",
       "19                     True                      100   \n",
       "20                     True                      100   \n",
       "21                     True                      100   \n",
       "\n",
       "                                         RecordedDate  \\\n",
       "0                                       Recorded Date   \n",
       "1   {\"ImportId\":\"recordedDate\",\"timeZone\":\"Europe/...   \n",
       "19                                2023-12-17 17:17:19   \n",
       "20                                2024-01-03 16:20:03   \n",
       "21                                2024-01-07 12:00:51   \n",
       "\n",
       "                  ResponseId  \\\n",
       "0                Response ID   \n",
       "1   {\"ImportId\":\"_recordId\"}   \n",
       "19         R_1R6hR2rt6cWkp6F   \n",
       "20         R_2RdYEoBoKYX29sa   \n",
       "21         R_2r8MhgLCoEhClwL   \n",
       "\n",
       "                                            StartDate                 Status  \\\n",
       "0                                          Start Date          Response Type   \n",
       "1   {\"ImportId\":\"startDate\",\"timeZone\":\"Europe/Ath...  {\"ImportId\":\"status\"}   \n",
       "19                                2023-12-17 17:06:58             IP Address   \n",
       "20                                2024-01-03 16:04:15             IP Address   \n",
       "21                                2024-01-07 11:50:39             IP Address   \n",
       "\n",
       "                   UserLanguage  ...                   war_jobStatusBefore  \\\n",
       "0                 User Language  ...  מצב תעסוקתי (בשגרה, לפני ה- 7/10/23)   \n",
       "1   {\"ImportId\":\"userLanguage\"}  ...          {\"ImportId\":\"QID1215542841\"}   \n",
       "19                           HE  ...                            משרה חלקית   \n",
       "20                           HE  ...                            משרה חלקית   \n",
       "21                           HE  ...                            משרה חלקית   \n",
       "\n",
       "                  war_newsConsumption  \\\n",
       "0   כמה זמן את/ה מבלה מול החדשות ביום   \n",
       "1        {\"ImportId\":\"QID1215542845\"}   \n",
       "19                          עד 2 שעות   \n",
       "20                          עד 2 שעות   \n",
       "21                          עד 2 שעות   \n",
       "\n",
       "                                    war_newsResources  \\\n",
       "0   מה מקור/ות החדשות העיקרי/ם שלך (ניתן לסמן כמה ...   \n",
       "1                        {\"ImportId\":\"QID1215542846\"}   \n",
       "19                        רדיו,אתרי חדשות,מדיה חברתית   \n",
       "20                                      רדיו,טלוויזיה   \n",
       "21                             אתרי חדשות,מדיה חברתית   \n",
       "\n",
       "                                        war_servForce  \\\n",
       "0   האם שרת/ת בשירות צבאי/מילואים/כוחות הביטחון מא...   \n",
       "1                        {\"ImportId\":\"QID1215660416\"}   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "\n",
       "                                       war_servForce2  \\\n",
       "0   אורך השירות במלחמה (יש לציין מספר שבועות או מס...   \n",
       "1                   {\"ImportId\":\"QID1215660417_TEXT\"}   \n",
       "19                                                NaN   \n",
       "20                                                NaN   \n",
       "21                                                NaN   \n",
       "\n",
       "                  war_servForce3                         war_servForce3other  \\\n",
       "0            תפקידי בשירות הינו:  במידה וסימנת אחר, נשמח שתפרט/י במידת האפשר   \n",
       "1   {\"ImportId\":\"QID1215660418\"}           {\"ImportId\":\"QID1215660419_TEXT\"}   \n",
       "19                           NaN                                         NaN   \n",
       "20                           NaN                                         NaN   \n",
       "21                           NaN                                         NaN   \n",
       "\n",
       "                                    war_worry2someone  \\\n",
       "0   האם יש א/נשים ספציפיים במעגל קרוב שאת/ה דואג/ת...   \n",
       "1                        {\"ImportId\":\"QID1215542851\"}   \n",
       "19                                                 כן   \n",
       "20                                                 כן   \n",
       "21                                                 כן   \n",
       "\n",
       "                                   war_worry2someone2  \\\n",
       "0   עד כמה הדאגה לא/נשים ספציפיים מעסיקה אותה במהל...   \n",
       "1                        {\"ImportId\":\"QID1215542852\"}   \n",
       "19                                          במידה רבה   \n",
       "20                                          במידה רבה   \n",
       "21                                      במידה בינונית   \n",
       "\n",
       "                                   war_worry2someone3  \n",
       "0   עד כמה הדאגה לא/נשים ספציפיים מפריעה לך לישון ...  \n",
       "1                        {\"ImportId\":\"QID1215542853\"}  \n",
       "19                                      במידה בינונית  \n",
       "20                                      במידה בינונית  \n",
       "21                                             כלל לא  \n",
       "\n",
       "[5 rows x 136 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We updated the questionniare on January 20, 2024. \n",
    "# So we saved the answers until the 20 and combined them to the new responders on the new version\n",
    "# For all old responders we added a None values on all the questions we add in the new version\n",
    "exp_data = pd.read_csv('questionnaire_raw_data/questionnaire_before_reserve_ques.csv')\n",
    "exp_data = exp_data.drop(exp_data[exp_data['DistributionChannel']=='preview'].index) # remove previews\n",
    "exp_data = exp_data.dropna(subset=['uniqeID']) # remove rows without uniqeID\n",
    "\n",
    "# this data is before the reserve questions added to the war questionnaire\n",
    "# exp_data_old_version = pd.read_csv('questionnaire_raw_data/questionnaire_May_17.csv')\n",
    "exp_data_old_version = pd.read_csv('questionnaire_raw_data/questionnaire_June_19.csv')\n",
    "exp_data_old_version = exp_data_old_version.drop(exp_data_old_version[exp_data_old_version['DistributionChannel']=='preview'].index) # remove previews\n",
    "exp_data_old_version = exp_data_old_version.dropna(subset=['uniqeID']) # remove rows without uniqeID\n",
    "\n",
    "data_df = exp_data.combine_first(exp_data_old_version)\n",
    "\n",
    "# fixing an error in the questionnaire in 1 question instead במידה מועטה רבה\n",
    "# to be במידה מועטה\n",
    "data_df.replace('במידה מועטה רבה', 'במידה מועטה', inplace=True)\n",
    "\n",
    "# participant number 132 confused and write 123 instead 132\n",
    "two_participants_id = data_df.loc[data_df['uniqeID']=='123']\n",
    "# fixing the confusion\n",
    "data_df['uniqeID'][data_df['EndDate']=='2024-01-29 19:18:49'] = '132'\n",
    "\n",
    "\n",
    "# participant 184 wrote with another ' ' so fixing it\n",
    "data_df.replace(' 184', '184',inplace=True)\n",
    "\n",
    "data_df.drop(67, inplace=True) # remove one questionnaire that was stucj to the participant and he start it over\n",
    "data_df[data_df['uniqeID']=='140'] # chacking that I see only one row with uniqeID of 140\n",
    "\n",
    "# participant 552 wrote in his uniqueID 551 instead\n",
    "two_participants_id = data_df.loc[data_df['uniqeID']=='551'] # finding the participants with same id\n",
    "data_df['uniqeID'][data_df['EndDate']=='2024-05-27 15:58:26'] = '552' # fixing the confusion\n",
    "\n",
    "# rename the column \"uniqeID\" bc a tipo\n",
    "data = data_df.rename(columns={'uniqeID': 'uniqueID'})\n",
    "\n",
    "# Display the updated new version dataframe\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9083d5b1",
   "metadata": {},
   "source": [
    "### Global Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45ade783",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ONLINE = pd.DataFrame()\n",
    "DF_reSMART = pd.DataFrame()\n",
    "DF_reBIOS = pd.DataFrame()\n",
    "DF_newBIOS = pd.DataFrame()\n",
    "DF_ONLINE_BIOS = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5a72f7",
   "metadata": {},
   "source": [
    "### Divide the participants according to experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7480451",
   "metadata": {},
   "outputs": [],
   "source": [
    "## need to filter out the relevant participant for each experiment:\n",
    "# Online (1__) # Smarting Return () # Smarting New () # BioSemi Return () # BioSemi New ()\n",
    "\n",
    "def assign_df_to_experiment(data):\n",
    "    id_signs = data['uniqueID'].str[0].unique()\n",
    "    \n",
    "    df_online = pd.DataFrame()\n",
    "    df_reSmart = pd.DataFrame()\n",
    "    df_newSmart = pd.DataFrame()\n",
    "    df_reBioS = pd.DataFrame()\n",
    "    df_newBioS = pd.DataFrame()\n",
    "    df_online_bios = pd.DataFrame()\n",
    "    \n",
    "    for id_sign in id_signs:\n",
    "        if id_sign == '1':\n",
    "            df_online = data.loc[data['uniqueID'].str.startswith('1'),:]\n",
    "#             df_online_bios = data.loc[data['uniqeID'].str.startswith('1'),:]\n",
    "            \n",
    "        elif id_sign == 'R':\n",
    "            df_reBioS = data.loc[data['uniqueID'].str.startswith('R'),:]\n",
    "            \n",
    "        elif id_sign == 'W':\n",
    "            df_reSmart = data.loc[data['uniqueID'].str.startswith('W'),:]\n",
    "            \n",
    "        elif id_sign == '5':\n",
    "            df_newBioS = data.loc[data['uniqueID'].str.startswith('5'),:]\n",
    "#             df_online_bios = data.loc[data['uniqeID'].str.startswith('5'),:]\n",
    "    df_online_bios = pd.concat([df_online, df_newBioS])\n",
    "            \n",
    "    return df_online, df_reSmart, df_reBioS, df_newBioS, df_online_bios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e7cdc0",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37df50b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DF_ONLINE, DF_reSMART, DF_reBIOS, DF_newBIOS, DF_ONLINE_BIOS = assign_df_to_experiment(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e19822b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate each DF to 6 dfs according to questionnaires\n",
    "\n",
    "def separate_to_6_dfs (df):\n",
    "    \n",
    "    df_demo = pd.DataFrame()    \n",
    "    df_war = pd.DataFrame()\n",
    "    df_dass = pd.DataFrame()\n",
    "    df_pcl = pd.DataFrame()\n",
    "    df_cope = pd.DataFrame()\n",
    "    df_asrs = pd.DataFrame()\n",
    "    \n",
    "    selected_demo_cols = df.filter(regex='^demo', axis=1)\n",
    "    selected_war_cols = df.filter(regex='^war', axis=1)\n",
    "    selected_dass_cols = df.filter(regex='^dass', axis=1)\n",
    "    selected_pcl_cols = df.filter(regex='^pcl', axis=1)\n",
    "    selected_cope_cols = df.filter(regex='^cope', axis=1)\n",
    "    selected_asrs_cols = df.filter(regex='^asrs', axis=1)\n",
    "\n",
    "    df_demo = df[selected_demo_cols.columns]\n",
    "    df_war = df[selected_war_cols.columns]\n",
    "    df_dass = df[selected_dass_cols.columns]\n",
    "    df_pcl = df[selected_pcl_cols.columns]\n",
    "    df_cope = df[selected_cope_cols.columns]\n",
    "    df_asrs = df[selected_asrs_cols.columns]\n",
    "    \n",
    "    return df_demo, df_war, df_dass, df_pcl, df_cope, df_asrs\n",
    "\n",
    "# Function to add the unique ID column to a DataFrame\n",
    "def add_uniqueID_column(dfs, main_df):\n",
    "    for df in dfs:\n",
    "        df.insert(0, 'uniqueID', main_df['uniqueID'])\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa99fef0",
   "metadata": {},
   "source": [
    "## Create separate DF for each questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eb6993",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "210f44bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_demo,all_war,all_dass,all_pcl,all_cope,all_asrs = separate_to_6_dfs(data)\n",
    "online_demo,online_war,online_dass,online_pcl,online_cope,online_asrs = separate_to_6_dfs(DF_ONLINE)\n",
    "onbio_demo,onbio_war,onbio_dass,onbio_pcl,onbio_cope,onbio_asrs = separate_to_6_dfs(DF_ONLINE_BIOS)\n",
    "add_uniqueID_column([onbio_demo,onbio_war,onbio_dass,onbio_pcl,onbio_cope,onbio_asrs], data)\n",
    "# list(onbio_dass['uniqueID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a0e2a4",
   "metadata": {},
   "source": [
    "#### Translation Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "26ad9e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "demo_dict = {'זכר': 'male', \n",
    "             'נקבה': 'female', \n",
    "             'כן': 'yes', \n",
    "             'לא': 'no', \n",
    "             'אחר': 'other', \n",
    "             'במהלך תואר': 'during academy',\n",
    "             'אקדמאית - בעל/ת תואר': 'academic',\n",
    "             'תיכונית': 'high scholl'}\n",
    "\n",
    "prior_diagnosis_categories = ['ADHD', 'PTSD', 'DASS', 'NO', 'OTHER'] # other include pain-killer and cancer drugs\n",
    "\n",
    "war_dict = {'מידי יום': 'Every day',\n",
    "            'בין פעמיים לשלוש בשבוע': 'Between 2-3 times a week',\n",
    "            'מעט': 'Few',\n",
    "            'בכלל לא': 'Not at all',\n",
    "            'כן': 'Yes',\n",
    "            'לא, אני מפונה ומתגורר/ת במלון/אכסנייה': 'No, I live in hotel',\n",
    "            'לא, אני מפונה ומתגורר/ת בבית אחר': 'No, I live at a house that not main',\n",
    "            'לא': 'No',\n",
    "            'באופן חלקי': 'Partially',\n",
    "            'לא צורך כלל': 'Not at all',\n",
    "            'עד 2 שעות': 'Two hours max',\n",
    "            'בין 2 ל- 3 שעות': 'Between 2 to 3 hours',\n",
    "            '5 שעות ויותר': 'At least 5 hours',\n",
    "            'כן, ואני עדיין בשירות פעיל': 'Yes, I still serve',\n",
    "            'כן, וסיימתי את שירותי': 'Yes, I finished my reserve',\n",
    "            'במידה רבה': 'A lot',\n",
    "            'במידה רבה מאוד': 'Extremely',\n",
    "            'במידה מועטה': 'Slightly',\n",
    "            'במידה בינונית': 'Moderately'}\n",
    "\n",
    "dass_dict = {'מתארת מאוד': 3,\n",
    "             'מתארת באופן ניכר': 2,\n",
    "             'מתארת באופן חלקי': 1,\n",
    "             'לא מתארת כלל': 0}\n",
    "\n",
    "pcl_dict = {'בכלל לא': 0,\n",
    "            'במידה מועטה': 1,\n",
    "            'במידה בינונית': 2,\n",
    "            'במידה רבה': 3, \n",
    "            'במידה רבה מאוד': 4}\n",
    "\n",
    "cope_dict = {'בכלל לא': 1,\n",
    "             'במידה מועטה': 2,\n",
    "             'במידה בינונית': 3,\n",
    "             'במידה רבה': 4}\n",
    "\n",
    "asrs_dict = {'אף פעם לא': 1,\n",
    "             'לעיתים רחוקות': 2,\n",
    "             'לפעמים': 3,\n",
    "             'לעיתים תכופות': 4,\n",
    "             'לעיתים תכופות מאוד': 5}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb08960",
   "metadata": {},
   "source": [
    "# Calculate the scores "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa30ab6",
   "metadata": {},
   "source": [
    "## Demo & War"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1843d785-2cc4-4a69-a4cd-17e7a6bd5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "onbio_prior_diagnosis = pd.DataFrame()\n",
    "onbio_prior_diagnosis['uniqueID'] = onbio_demo['uniqueID']\n",
    "#onbio_demo[['uniqueID', 'demo_illness','demo_illness_details','demo_meds','demo_meds_details']][55:105]\n",
    "onbio_prior_diagnosis['prior_diagnosis'] = ['no']*len(onbio_prior_diagnosis)\n",
    "\n",
    "# -------------------------------------\n",
    "# handel with prior diagnosis manually\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '104', 'prior_diagnosis'] = 'DASS'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '115', 'prior_diagnosis'] = 'DASS'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '116', 'prior_diagnosis'] = 'other'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '121', 'prior_diagnosis'] = 'PTSD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '129', 'prior_diagnosis'] = 'other'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '132', 'prior_diagnosis'] = 'PTSD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '142', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '170', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '180', 'prior_diagnosis'] = 'DASS'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '182', 'prior_diagnosis'] = 'DASS'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '183', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '184', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '187', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '155', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '163', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '164', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '196', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '502', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '503', 'prior_diagnosis'] = 'PTSD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '504', 'prior_diagnosis'] = 'DASS'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '505', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '509', 'prior_diagnosis'] = 'other'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '519', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '522', 'prior_diagnosis'] = 'ADHD'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '530', 'prior_diagnosis'] = 'DASS'\n",
    "onbio_prior_diagnosis.loc[onbio_prior_diagnosis['uniqueID'] == '533', 'prior_diagnosis'] = 'DASS'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d00d3df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(149, 149)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_onbio_demo = onbio_demo.replace(demo_dict)\n",
    "eng_onbio_war = onbio_war.replace(war_dict)\n",
    "len(eng_onbio_demo), len(eng_onbio_war)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2afe0dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset_demo_war = pd.DataFrame()\n",
    "dataset_demo_war['uniqueID'] = eng_onbio_demo['uniqueID']\n",
    "eng_onbio_demo['demo_numChildren'] = pd.to_numeric(eng_onbio_demo['demo_numChildren'], errors='coerce')\n",
    "\n",
    "# copy the demo data\n",
    "dataset_demo_war = pd.merge(dataset_demo_war, onbio_prior_diagnosis, on='uniqueID')\n",
    "\n",
    "dataset_demo_war = pd.merge(dataset_demo_war,eng_onbio_demo[['uniqueID','demo_age','demo_geneder','demo_eduction','demo_armyServed',\n",
    "                                                             'demo_numChildren']], on='uniqueID')\n",
    "\n",
    "dataset_demo_war = dataset_demo_war.rename(columns={'demo_age': 'age', 'demo_geneder': 'gender', 'demo_eduction':'education',\n",
    "                                                    'demo_armyServed':'army_serve','demo_numChildren':'has_kids'})\n",
    "\n",
    "# create the has kids a categorical assessment instad of numerical\n",
    "dataset_demo_war['has_kids'] = np.where(dataset_demo_war['has_kids'] > 0, 'yes', dataset_demo_war['has_kids'])\n",
    "dataset_demo_war['has_kids'] = np.where(dataset_demo_war['has_kids'] == 0, 'no', dataset_demo_war['has_kids'])\n",
    "\n",
    "# copy the war data\n",
    "dataset_demo_war = pd.merge(dataset_demo_war,eng_onbio_war[['uniqueID','war_alarmFreq_start','war_servForce','war_familiarFallens',\n",
    "                                                             'war_jobRelate2war','war_jobSimilarities','war_newsConsumption','war_addressNow',\n",
    "                                                             'war_familiarkidnap','war_worry2someone']], on='uniqueID')\n",
    "\n",
    "dataset_demo_war = dataset_demo_war.rename(columns={'war_alarmFreq_start': 'sirens', 'war_servForce': 'reserve_duty', \n",
    "                                                    'war_familiarFallens':'knows_victims','war_jobRelate2war':'work_relate2war', \n",
    "                                                    'war_jobSimilarities':'work_changes','war_newsConsumption':'news_consumption',\n",
    "                                                    'war_addressNow':'live_in_your_place','war_familiarkidnap':'knows_hostages',\n",
    "                                                    'war_worry2someone':'worry_to_someone'})\n",
    "\n",
    "# I created also war only dataset in case needed\n",
    "dataset_war = pd.DataFrame()\n",
    "dataset_war['uniqueID'] = eng_onbio_demo['uniqueID']\n",
    "dataset_war = pd.merge(dataset_war,dataset_demo_war[['uniqueID','sirens','reserve_duty','knows_victims','work_relate2war','work_changes',\n",
    "                                                     'news_consumption','live_in_your_place','knows_hostages','worry_to_someone']],\n",
    "                        on='uniqueID')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "387d6eb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uniqueID</th>\n",
       "      <th>prior_diagnosis</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>army_serve</th>\n",
       "      <th>has_kids</th>\n",
       "      <th>sirens</th>\n",
       "      <th>reserve_duty</th>\n",
       "      <th>knows_victims</th>\n",
       "      <th>work_relate2war</th>\n",
       "      <th>work_changes</th>\n",
       "      <th>news_consumption</th>\n",
       "      <th>live_in_your_place</th>\n",
       "      <th>knows_hostages</th>\n",
       "      <th>worry_to_someone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>no</td>\n",
       "      <td>25</td>\n",
       "      <td>female</td>\n",
       "      <td>during academy</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Few</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Extremely</td>\n",
       "      <td>Two hours max</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>no</td>\n",
       "      <td>27</td>\n",
       "      <td>female</td>\n",
       "      <td>academic</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Few</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Extremely</td>\n",
       "      <td>Two hours max</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>no</td>\n",
       "      <td>30</td>\n",
       "      <td>male</td>\n",
       "      <td>academic</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Between 2-3 times a week</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A lot</td>\n",
       "      <td>Between 2 to 3 hours</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>104</td>\n",
       "      <td>DASS</td>\n",
       "      <td>28</td>\n",
       "      <td>male</td>\n",
       "      <td>academic</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not at all</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A lot</td>\n",
       "      <td>Between 2 to 3 hours</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>105</td>\n",
       "      <td>no</td>\n",
       "      <td>27</td>\n",
       "      <td>female</td>\n",
       "      <td>academic</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Few</td>\n",
       "      <td>NaN</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A lot</td>\n",
       "      <td>Two hours max</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>553</td>\n",
       "      <td>no</td>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>during academy</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Few</td>\n",
       "      <td>Yes, I finished my reserve</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Slightly</td>\n",
       "      <td>Two hours max</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>554</td>\n",
       "      <td>no</td>\n",
       "      <td>46</td>\n",
       "      <td>male</td>\n",
       "      <td>academic</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Between 2-3 times a week</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A lot</td>\n",
       "      <td>Two hours max</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>555</td>\n",
       "      <td>no</td>\n",
       "      <td>23</td>\n",
       "      <td>male</td>\n",
       "      <td>during academy</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Every day</td>\n",
       "      <td>Yes, I finished my reserve</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not at all</td>\n",
       "      <td>Not at all</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>556</td>\n",
       "      <td>no</td>\n",
       "      <td>25</td>\n",
       "      <td>male</td>\n",
       "      <td>during academy</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Between 2-3 times a week</td>\n",
       "      <td>Yes, I still serve</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Extremely</td>\n",
       "      <td>Not at all</td>\n",
       "      <td>No, I live at a house that not main</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>557</td>\n",
       "      <td>no</td>\n",
       "      <td>24</td>\n",
       "      <td>male</td>\n",
       "      <td>during academy</td>\n",
       "      <td>yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Between 2-3 times a week</td>\n",
       "      <td>Yes, I still serve</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>A lot</td>\n",
       "      <td>Not at all</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    uniqueID prior_diagnosis age  gender       education army_serve has_kids  \\\n",
       "0        101              no  25  female  during academy        yes      0.0   \n",
       "1        102              no  27  female        academic        yes      0.0   \n",
       "2        103              no  30    male        academic        yes      0.0   \n",
       "3        104            DASS  28    male        academic        yes      0.0   \n",
       "4        105              no  27  female        academic        yes      0.0   \n",
       "..       ...             ...  ..     ...             ...        ...      ...   \n",
       "144      553              no  24    male  during academy        yes      0.0   \n",
       "145      554              no  46    male        academic        yes      0.0   \n",
       "146      555              no  23    male  during academy        yes      0.0   \n",
       "147      556              no  25    male  during academy        yes      0.0   \n",
       "148      557              no  24    male  during academy        yes      0.0   \n",
       "\n",
       "                       sirens                reserve_duty knows_victims  \\\n",
       "0                         Few                         NaN            No   \n",
       "1                         Few                         NaN            No   \n",
       "2    Between 2-3 times a week                         NaN            No   \n",
       "3                  Not at all                         NaN            No   \n",
       "4                         Few                         NaN            No   \n",
       "..                        ...                         ...           ...   \n",
       "144                       Few  Yes, I finished my reserve           Yes   \n",
       "145  Between 2-3 times a week                          No            No   \n",
       "146                 Every day  Yes, I finished my reserve           Yes   \n",
       "147  Between 2-3 times a week          Yes, I still serve           Yes   \n",
       "148  Between 2-3 times a week          Yes, I still serve            No   \n",
       "\n",
       "    work_relate2war work_changes      news_consumption  \\\n",
       "0                No    Extremely         Two hours max   \n",
       "1                No    Extremely         Two hours max   \n",
       "2                No        A lot  Between 2 to 3 hours   \n",
       "3                No        A lot  Between 2 to 3 hours   \n",
       "4                No        A lot         Two hours max   \n",
       "..              ...          ...                   ...   \n",
       "144              No     Slightly         Two hours max   \n",
       "145              No        A lot         Two hours max   \n",
       "146              No   Not at all            Not at all   \n",
       "147              No    Extremely            Not at all   \n",
       "148              No        A lot            Not at all   \n",
       "\n",
       "                      live_in_your_place knows_hostages worry_to_someone  \n",
       "0                                    Yes             No              Yes  \n",
       "1                                    Yes             No              Yes  \n",
       "2                                    Yes             No              Yes  \n",
       "3                                    Yes             No               No  \n",
       "4                                    Yes             No              Yes  \n",
       "..                                   ...            ...              ...  \n",
       "144                                  Yes             No              Yes  \n",
       "145                                  Yes             No              Yes  \n",
       "146                                  Yes             No              Yes  \n",
       "147  No, I live at a house that not main             No              Yes  \n",
       "148                                  Yes             No              Yes  \n",
       "\n",
       "[149 rows x 16 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_demo_war\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aedd35e4",
   "metadata": {},
   "source": [
    "## Subjective Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a058aff",
   "metadata": {},
   "source": [
    "### DASS & PCL & ASRS & COPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "201e4271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# ------------------ FUNCTIONS --------------------\n",
    "# -------------------------------------------------\n",
    "\n",
    "'''\n",
    " This function calculate the score for the DASS questionnaire \n",
    " by sum all the relevant questions for each measure\n",
    " (1)Anxiety (2)Depression (3)Stress\n",
    " Input: the dataframe of the dass \n",
    " Output: a new dataframe includs 3 columns one for each measure\n",
    "'''\n",
    "def calc_dass_scores(original_df):\n",
    "    \n",
    "    # Identify the groups based on column names\n",
    "    anxiety_cols = [col for col in original_df.columns if col.startswith('dass_a')]\n",
    "    depression_cols = [col for col in original_df.columns if col.startswith('dass_d')]\n",
    "    stress_cols = [col for col in original_df.columns if col.startswith('dass_s')]\n",
    "\n",
    "    # Calculate the sum for each group\n",
    "    score_df = pd.DataFrame()\n",
    "    score_df['uniqueID'] = original_df['uniqueID']\n",
    "    score_df['anxiety'] = original_df[anxiety_cols].sum(axis=1)\n",
    "    score_df['depression'] = original_df[depression_cols].sum(axis=1)\n",
    "    score_df['stress'] = original_df[stress_cols].sum(axis=1)\n",
    "    \n",
    "    return score_df\n",
    "\n",
    "\n",
    "'''\n",
    " This function calculate the score for the PCL questionnaire \n",
    " by sum all the questions\n",
    " Input: the dataframe of the pcl \n",
    " Output: a new dataframe includs a column of the score\n",
    "'''\n",
    "def calc_pcl_score(original_df):\n",
    "    pcl_cols = [col for col in original_df.columns if col.startswith('pcl')]\n",
    "    score_df = pd.DataFrame()\n",
    "    score_df['uniqueID'] = original_df['uniqueID']\n",
    "    score_df['pcl'] = original_df[pcl_cols].sum(axis=1)\n",
    "\n",
    "    return score_df\n",
    "\n",
    "\n",
    "'''\n",
    " This function calculate the score for the COPE questionnaire \n",
    " by average the relevant questions for each cope mechanism\n",
    " (1)Problem focuser (2)Emotional focuser (3)Avoidance\n",
    " Input: the dataframe of the cope\n",
    " Output: a new dataframe includs 3 columns one for each measure\n",
    "'''\n",
    "def calc_cope_scores(original_df):\n",
    "    \n",
    "    # Identify the groups based on column names\n",
    "    problems_cols = [col for col in original_df.columns if col.startswith('cope_p')]\n",
    "    emotional_cols = [col for col in original_df.columns if col.startswith('cope_e')]\n",
    "    avoidance_cols = [col for col in original_df.columns if col.startswith('cope_a')]\n",
    "\n",
    "    # Calculate the sum for each group\n",
    "    score_df = pd.DataFrame()\n",
    "    score_df['uniqueID'] = original_df['uniqueID']\n",
    "    score_df['problems'] = round((original_df[problems_cols].sum(axis=1))/(len(problems_cols)),2)\n",
    "    score_df['emotional'] = round((original_df[emotional_cols].sum(axis=1))/(len(emotional_cols)),2)\n",
    "    score_df['avoidance'] = round((original_df[avoidance_cols].sum(axis=1))/(len(avoidance_cols)),2)\n",
    "    \n",
    "    return score_df\n",
    "\n",
    "\n",
    "'''\n",
    " This function calculate the score for the ASRS questionnaire \n",
    " by sum all the questions\n",
    " Input: the dataframe of the asrs \n",
    " Output: a new dataframe includs a column of the score\n",
    "'''\n",
    "def calc_asrs_score(original_df):\n",
    "    asrs_cols = [col for col in original_df.columns if col.startswith('asrs')]\n",
    "    score_df = pd.DataFrame()\n",
    "    score_df['uniqueID'] = original_df['uniqueID']\n",
    "    score_df['asrs'] = original_df[asrs_cols].sum(axis=1)\n",
    "\n",
    "    return score_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e47a5437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the second dataset table - dataset_subjective\n",
    "dataset_subjective = pd.DataFrame()\n",
    "dataset_subjective['uniqueID'] = onbio_dass['uniqueID']\n",
    "numeric_onbio_dass = onbio_dass.replace(dass_dict)\n",
    "numeric_onbio_pcl = onbio_pcl.replace(pcl_dict)\n",
    "numeric_onbio_cope = onbio_cope.replace(cope_dict)\n",
    "numeric_onbio_asrs = onbio_asrs.replace(asrs_dict)\n",
    "\n",
    "\n",
    "\n",
    "# insert all the scores to the dataset\n",
    "dass_scores = calc_dass_scores(numeric_onbio_dass)\n",
    "dataset_subjective = pd.merge(dataset_subjective, dass_scores, on='uniqueID')\n",
    "\n",
    "pcl_score = calc_pcl_score(numeric_onbio_pcl)\n",
    "dataset_subjective = pd.merge(dataset_subjective, pcl_score, on='uniqueID')\n",
    "\n",
    "asrs_score = calc_asrs_score(numeric_onbio_asrs)\n",
    "dataset_subjective = pd.merge(dataset_subjective, asrs_score, on='uniqueID')\n",
    "\n",
    "cope_scores = calc_cope_scores(numeric_onbio_cope)\n",
    "dataset_subjective = pd.merge(dataset_subjective, cope_scores, on='uniqueID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "dbbe21ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "145\n"
     ]
    }
   ],
   "source": [
    "list(dataset_subjective['uniqueID'])\n",
    "\n",
    "# need to remove 517, 525, 512\n",
    "dataset_subjective_new = dataset_subjective.drop(dataset_subjective[dataset_subjective['uniqueID'] == '517'].index)\n",
    "dataset_subjective_new = dataset_subjective_new.drop(dataset_subjective_new[dataset_subjective_new['uniqueID'] == '512'].index)\n",
    "dataset_subjective_new = dataset_subjective_new.drop(dataset_subjective_new[dataset_subjective_new['uniqueID'] == '525'].index)\n",
    "dataset_subjective_new = dataset_subjective_new.drop(dataset_subjective_new[dataset_subjective_new['uniqueID'] == '505'].index)\n",
    "\n",
    "dataset_war_new = dataset_war.drop(dataset_war[dataset_war['uniqueID'] == '517'].index)\n",
    "dataset_war_new = dataset_war_new.drop(dataset_war_new[dataset_war_new['uniqueID'] == '512'].index)\n",
    "dataset_war_new = dataset_war_new.drop(dataset_war_new[dataset_war_new['uniqueID'] == '525'].index)\n",
    "dataset_war_new = dataset_war_new.drop(dataset_war_new[dataset_war_new['uniqueID'] == '505'].index)\n",
    "\n",
    "# dua 31 May we have until participant-541\n",
    "print(len(list(dataset_subjective_new['uniqueID'])))\n",
    "print(len(list(dataset_war_new['uniqueID'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c010092f",
   "metadata": {},
   "source": [
    "# Read Flanker data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "52f76070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the task csv for online and bios\n",
    "'''\n",
    "This function loop over all the csv files in a source folder\n",
    "and copied them to the destionation folder\n",
    "Input: (1)source path string (2)destionattion path string\n",
    "Output:\n",
    "'''\n",
    "def copy_task_data(source_path, destionation_path):\n",
    "    current_folder_path = source_path\n",
    "    destination_folder = destionation_path\n",
    "    csv_files_list = os.listdir(current_folder_path)\n",
    "    for csv_file in csv_files_list:\n",
    "        # get the uniqueID name from the original file name (3 letters number)\n",
    "        new_file_name = csv_file[:3]\n",
    "        shutil.copyfile(f'{current_folder_path}/{csv_file}', f'{destination_folder}/{new_file_name}.csv')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4d858ecc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save this data on my computer, if want to save in lab need to be change the folder name\n",
    "# online_and_bio_task_data = '/Users/prague/Documents/NeuroScience-Master/Elana_Lab/War_Research/code/analysis/online_and_bio_task_data'\n",
    "online_and_bio_task_data = '/Users/pagueoshri/Documents/NeuroScience-Master/Elana_Lab/War_Research/code/analysis/online_and_bio_task_data'\n",
    "if not os.path.exists(online_and_bio_task_data):\n",
    "    os.makedirs(online_and_bio_task_data)\n",
    "\n",
    "\n",
    "source_path_online = '/Users/pagueoshri/Documents/NeuroScience-Master/Elana_Lab/War_Research/code/analysis/task_raw_data/online_only'\n",
    "source_path_bio = '/Users/pagueoshri/Documents/NeuroScience-Master/Elana_Lab/War_Research/code/analysis/task_raw_data/bio'\n",
    "\n",
    "# copy the data from online and bio to the same destionation folder\n",
    "copy_task_data(source_path_online, online_and_bio_task_data)\n",
    "copy_task_data(source_path_bio, online_and_bio_task_data)\n",
    "\n",
    "# count the number of csv files in the new folder\n",
    "onbio_csv_files_list = os.listdir(online_and_bio_task_data)\n",
    "\n",
    "# dua 31 May we have until participant-538\n",
    "onbio_csv_files_list = sorted(onbio_csv_files_list)\n",
    "onbio_csv_files_list.remove('.DS_Store')\n",
    "onbio_csv_files_list.remove('505.csv')\n",
    "len(onbio_csv_files_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82120a64",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6c879b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "16     right\n",
       "17     right\n",
       "18     right\n",
       "19     right\n",
       "20      left\n",
       "       ...  \n",
       "268    right\n",
       "269    right\n",
       "270    right\n",
       "271     left\n",
       "272      NaN\n",
       "Name: trial_resp.keys, Length: 257, dtype: object"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Num_Trials = 256\n",
    "print(type(onbio_csv_files_list[1]))\n",
    "t = pd.read_csv(f'online_and_bio_task_data/{onbio_csv_files_list[1]}')\n",
    "t['trial_resp.keys'][16:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c4b1f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function getting the same group of random trials \n",
    "for the no-vol condition, in order to get the same \n",
    "group size of analysis\n",
    "Input: csv file\n",
    "Output: list of indeces from the no-vol trials\n",
    "'''\n",
    "def get_random_trials_of_noVol(csv_file):\n",
    "    tone_group = csv_file[(csv_file['vol'] == 1)]\n",
    "    no_tone_group = csv_file[(csv_file['vol'] == 0)]\n",
    "    \n",
    "    # rand trials as the number of 25% trials with tone\n",
    "    seed(1) # choose the same trials accross all the random\n",
    "    random_trials = randint(1, len(no_tone_group), len(tone_group))\n",
    "    \n",
    "    return random_trials\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "This function reading the relevant information from each csv\n",
    "'''\n",
    "def fill_task_table_variables(csv_file, unique_id):\n",
    "#     print(csv_file['trial_resp.keys'])\n",
    "    destination_df = pd.DataFrame()\n",
    "    \n",
    "    num_misses = csv_file['trial_resp.keys'].isna().sum()\n",
    "#     print(num_misses)\n",
    "    mean_rt = np.mean(csv_file['trial_resp.rt'])\n",
    "    acc = (np.sum(csv_file['trial_resp.corr']) / (Num_Trials)) * 100 # accuracy percentage\n",
    "    \n",
    "    con_rows = csv_file.loc[(csv_file['conditions'].eq('con_right') | csv_file['conditions'].eq('con_left'))]\n",
    "    con_rt = np.mean(con_rows['trial_resp.rt'])\n",
    "    con_acc = (np.sum(con_rows['trial_resp.corr']))/(len(con_rows)) * 100\n",
    "    \n",
    "    incon_rows = csv_file.loc[csv_file['conditions'].eq('incon_right') | csv_file['conditions'].eq('incon_left')]\n",
    "    incon_rt = np.mean(incon_rows['trial_resp.rt'])\n",
    "    incon_acc = (np.sum(incon_rows['trial_resp.corr']))/(len(incon_rows)) * 100\n",
    "    \n",
    "    vol_rows = csv_file.loc[csv_file['vol'] == 1]\n",
    "    vol_rt = np.mean(vol_rows['trial_resp.rt'])\n",
    "    vol_acc = (np.sum(vol_rows['trial_resp.corr']))/(len(vol_rows)) * 100\n",
    "    \n",
    "    random_trials = get_random_trials_of_noVol(csv_file)\n",
    "    novol_rows = csv_file[(csv_file['vol'] == 0)].iloc[random_trials]\n",
    "    novol_rt = np.mean(novol_rows['trial_resp.rt'])\n",
    "    novol_acc = (np.sum(novol_rows['trial_resp.corr']))/(len(novol_rows)) * 100\n",
    "    \n",
    "    phasic_alertness = novol_rt - vol_rt\n",
    "    executive_control = con_rt - incon_rt\n",
    "\n",
    "    # find the union group of the novol/vol with con/incon \n",
    "    # in order to get comparisions of phasic alertness and executive control\n",
    "    vol_con_rows = pd.merge(con_rows, vol_rows, how='inner')\n",
    "    novol_con_rows = pd.merge(con_rows, novol_rows, how='inner')\n",
    "    vol_incon_rows = pd.merge(incon_rows, vol_rows, how='inner')\n",
    "    novol_incon_rows = pd.merge(incon_rows, novol_rows, how='inner')\n",
    "    \n",
    "    vol_con_rt = np.mean(vol_con_rows['trial_resp.rt'])\n",
    "    novol_con_rt = np.mean(novol_con_rows['trial_resp.rt'])\n",
    "    vol_incon_rt = np.mean(vol_incon_rows['trial_resp.rt'])\n",
    "    novol_incon_rt = np.mean(novol_incon_rows['trial_resp.rt'])\n",
    "\n",
    "    phasic_alertness_con = novol_con_rt - vol_con_rt\n",
    "    phasic_alertness_incon = novol_incon_rt - vol_incon_rt\n",
    "    executive_control_vol = vol_con_rt - vol_incon_rt\n",
    "    executive_control_novol = novol_con_rt - novol_incon_rt\n",
    "\n",
    "    \n",
    "    task_dict = {'uniqueID':unique_id, 'misses':num_misses, 'mean_RT':mean_rt, 'accuracy': acc,\n",
    "                 'con_RT':con_rt, 'incon_RT':incon_rt, 'vol_RT':vol_rt, 'novol_RT':novol_rt,\n",
    "                 'con_Acc':con_acc, 'incon_Acc':incon_acc, 'vol_Acc':vol_acc, 'novol_Acc':novol_acc,\n",
    "                 'phasic_alertness': phasic_alertness, 'executive_control': executive_control,\n",
    "                 'phasic_alertness_con': phasic_alertness_con, 'phasic_alertness_incon': phasic_alertness_incon,\n",
    "                 'executive_control_vol': executive_control_vol, 'executive_control_novol': executive_control_novol}\n",
    "    \n",
    "    return task_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0ab89106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145\n",
      "6\n",
      "139\n"
     ]
    }
   ],
   "source": [
    "# create a new table for the task data\n",
    "columns=['uniqueID','misses','mean_RT','accuracy','con_RT','incon_RT','vol_RT','novol_RT','con_Acc',\n",
    "         'incon_Acc','vol_Acc','novol_Acc','phasic_alertness', 'executive_control', 'phasic_alertness_con', \n",
    "         'phasic_alertness_incon','executive_control_vol', 'executive_control_novol']\n",
    "index=list(dataset_subjective_new['uniqueID'])\n",
    "dataset_objective_task = pd.DataFrame(columns=columns,index=index)\n",
    "dataset_objective_task['uniqueID'] = index\n",
    "dataset_objective_task = dataset_objective_task.drop(dataset_objective_task[dataset_objective_task['uniqueID'] == '505'].index)\n",
    "\n",
    "\n",
    "# loop over all the csv files and fill in the table\n",
    "for csv_file in onbio_csv_files_list:\n",
    "    csv_unique_id = csv_file[:3]\n",
    "    curr_csv_file = pd.read_csv(f'online_and_bio_task_data/{csv_file}')\n",
    "    curr_dict = fill_task_table_variables(curr_csv_file[16:],csv_unique_id)\n",
    "    dataset_objective_task.loc[curr_dict['uniqueID']] = pd.Series(curr_dict)\n",
    "\n",
    "print(len(dataset_objective_task))\n",
    "print(len(dataset_objective_task[dataset_objective_task['misses'] >= 20]))\n",
    "# Remove outlier where misses more than 20 trials\n",
    "# dataset_objective_task = dataset_objective_task.drop(dataset_objective_task[dataset_objective_task['misses'] >= 10].index)\n",
    "dataset_objective_task = dataset_objective_task.drop(dataset_objective_task[dataset_objective_task['misses'] >= 20].index)\n",
    "# dataset_objective_task = dataset_objective_task.drop(dataset_objective_task[dataset_objective_task['misses'] >= 35].index)\n",
    "print(len(dataset_objective_task))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "682cb8a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_objective_task\n",
    "\n",
    "# Need to remove participants that didn't use headphones in the task:\n",
    "# uniqueIDs: 101, 102, 185\n",
    "dataset_objective_task_new = dataset_objective_task.drop(dataset_objective_task[dataset_objective_task['uniqueID'] == '101'].index)\n",
    "dataset_objective_task_new = dataset_objective_task_new.drop(dataset_objective_task_new[dataset_objective_task_new['uniqueID'] == '102'].index)\n",
    "dataset_objective_task_new = dataset_objective_task_new.drop(dataset_objective_task_new[dataset_objective_task_new['uniqueID'] == '185'].index)\n",
    "\n",
    "len(dataset_objective_task_new['uniqueID'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd5da2a-4b5b-405f-818a-e0532070b724",
   "metadata": {},
   "source": [
    "## Save 3 DataFrames as csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4f41076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_demo_war.to_csv('demo_war_table.csv')\n",
    "dataset_war_new.to_csv('war_table.csv')\n",
    "dataset_subjective_new.to_csv('subjective_table.csv')\n",
    "dataset_objective_task_new.to_csv('objective_task_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e2dc95-555c-406b-af06-c40271144cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
